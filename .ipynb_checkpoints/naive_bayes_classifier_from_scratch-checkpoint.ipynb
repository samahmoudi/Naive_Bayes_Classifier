{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72c11ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.model_selection as sms\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd1e584a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                           int64\n",
      "diagnosis                   object\n",
      "radius_mean                float64\n",
      "texture_mean               float64\n",
      "perimeter_mean             float64\n",
      "area_mean                  float64\n",
      "smoothness_mean            float64\n",
      "compactness_mean           float64\n",
      "concavity_mean             float64\n",
      "concave points_mean        float64\n",
      "symmetry_mean              float64\n",
      "fractal_dimension_mean     float64\n",
      "radius_se                  float64\n",
      "texture_se                 float64\n",
      "perimeter_se               float64\n",
      "area_se                    float64\n",
      "smoothness_se              float64\n",
      "compactness_se             float64\n",
      "concavity_se               float64\n",
      "concave points_se          float64\n",
      "symmetry_se                float64\n",
      "fractal_dimension_se       float64\n",
      "radius_worst               float64\n",
      "texture_worst              float64\n",
      "perimeter_worst            float64\n",
      "area_worst                 float64\n",
      "smoothness_worst           float64\n",
      "compactness_worst          float64\n",
      "concavity_worst            float64\n",
      "concave points_worst       float64\n",
      "symmetry_worst             float64\n",
      "fractal_dimension_worst    float64\n",
      "Unnamed: 32                float64\n",
      "dtype: object\n",
      "id                           0\n",
      "diagnosis                    0\n",
      "radius_mean                  0\n",
      "texture_mean                 0\n",
      "perimeter_mean               0\n",
      "area_mean                    0\n",
      "smoothness_mean              0\n",
      "compactness_mean             0\n",
      "concavity_mean               0\n",
      "concave points_mean          0\n",
      "symmetry_mean                0\n",
      "fractal_dimension_mean       0\n",
      "radius_se                    0\n",
      "texture_se                   0\n",
      "perimeter_se                 0\n",
      "area_se                      0\n",
      "smoothness_se                0\n",
      "compactness_se               0\n",
      "concavity_se                 0\n",
      "concave points_se            0\n",
      "symmetry_se                  0\n",
      "fractal_dimension_se         0\n",
      "radius_worst                 0\n",
      "texture_worst                0\n",
      "perimeter_worst              0\n",
      "area_worst                   0\n",
      "smoothness_worst             0\n",
      "compactness_worst            0\n",
      "concavity_worst              0\n",
      "concave points_worst         0\n",
      "symmetry_worst               0\n",
      "fractal_dimension_worst      0\n",
      "Unnamed: 32                569\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Read the data from the CSV file\n",
    "file_path = r'data.csv'  \n",
    "raw_dataset = pd.read_csv(file_path)\n",
    "# Print the data types of each column in the dataset; all should be of float type\n",
    "print (raw_dataset.dtypes)\n",
    "# Check for missing values in each column\n",
    "print(raw_dataset.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d8b73fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'M': 0, 'B': 1}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop noninformative column (null column and id column)\n",
    "raw_dataset = raw_dataset.drop(['Unnamed: 32','id'], axis=1) #this column is NaN \n",
    "\n",
    "# Print the entire dataset, including all rows and columns\n",
    "raw_dataset\n",
    "\n",
    "# Converting qualitative labels to numerical ones \n",
    "def label_to_numeric(column):\n",
    "    if column.dtype == 'object':\n",
    "        unique_labels, _ = pd.factorize(column)\n",
    "        return pd.Series(unique_labels, index=column.index)\n",
    "    \n",
    "    return column\n",
    "\n",
    "dataset = raw_dataset.apply(label_to_numeric)\n",
    "# Understanding which number is attributed to which label\n",
    "label_map = dict(zip(raw_dataset['diagnosis'], dataset['diagnosis']))\n",
    "label_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "55de84ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = list(set(dataset.columns) - {'diagnosis'}) # Feature label extraction\n",
    "label = dataset['diagnosis']\n",
    "data = dataset[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f2597c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_trainset, data_testset, label_trainset, label_testset = sms.train_test_split(data, label, test_size=0.2)\n",
    "\n",
    "#Training Guassian naive baise\n",
    "naive_bayes_model = GaussianNB()\n",
    "naive_bayes_model.fit(data_trainset, label_trainset)\n",
    "prediction = naive_bayes_model.predict(data_testset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "36252fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9298245614035088 \n",
      "\n",
      "\n",
      "Category M\n",
      "                     Actual Positive  Actual Negative\n",
      "Predicted Positive               36                5\n",
      "Predicted Negative                3               70 \n",
      "\n",
      " Category B\n",
      "                     Actual Positive  Actual Negative\n",
      "Predicted Positive               70                5\n",
      "Predicted Negative                3               36 \n",
      "\n",
      "\n",
      "Category M Precision = 0.9230769230769231 Recall = 0.8780487804878049\n",
      "Category B Precision = 0.9333333333333333 Recall = 0.958904109589041\n"
     ]
    }
   ],
   "source": [
    "#parameters\n",
    "accuracy = accuracy_score(label_testset, prediction)\n",
    "print(\"Accuracy:\" ,accuracy, '\\n\\n')\n",
    "\n",
    "Comparison = pd.concat([label_testset.reset_index(drop=True) ,pd.Series(prediction)], axis=1)\n",
    "Comparison.rename(columns={0: 'classifier'}, inplace=True)\n",
    "Comparison\n",
    "\n",
    "# postitive = 'M' , negative = 'B'\n",
    "M_TP = ((Comparison['diagnosis'] == 0) & (Comparison['classifier'] == 0)).sum()\n",
    "M_FN = ((Comparison['diagnosis'] == 0) & (Comparison['classifier'] == 1)).sum()\n",
    "M_TN = ((Comparison['diagnosis'] == 1) & (Comparison['classifier'] == 1)).sum()\n",
    "M_FP = ((Comparison['diagnosis'] == 1) & (Comparison['classifier'] == 0)).sum()\n",
    "\n",
    "M_confusion_matarix = confusion_matrix(label_testset, prediction)\n",
    "\n",
    "M_confusion_matarix = pd.DataFrame(M_confusion_matarix, columns=['Actual Positive', 'Actual Negative'], index=['Predicted Positive', 'Predicted Negative'])\n",
    "\n",
    "# postitive = 'B' , negative = 'M'\n",
    "B_TP = ((Comparison['diagnosis'] == 1) & (Comparison['classifier'] == 1)).sum()\n",
    "B_FN = ((Comparison['diagnosis'] == 1) & (Comparison['classifier'] == 0)).sum()\n",
    "B_TN = ((Comparison['diagnosis'] == 0) & (Comparison['classifier'] == 0)).sum()\n",
    "B_FP = ((Comparison['diagnosis'] == 0) & (Comparison['classifier'] == 1)).sum()\n",
    "\n",
    "B_confusion_matarix = {\n",
    "    'Actual Positive': [B_TP, B_FN],\n",
    "    'Actual Negative': [B_FP, B_TN],\n",
    "}\n",
    "\n",
    "\n",
    "B_confusion_matarix = pd.DataFrame(B_confusion_matarix, columns=['Actual Positive', 'Actual Negative'], index=['Predicted Positive', 'Predicted Negative'])\n",
    "\n",
    "print('Category M\\n' ,M_confusion_matarix,'\\n\\n','Category B\\n', B_confusion_matarix, '\\n\\n')\n",
    "\n",
    "M_Precision = M_TP/(M_TP+M_FP)\n",
    "M_Recall = M_TP/(M_TP+M_FN)\n",
    "\n",
    "print('Category M' ,'Precision =', M_Precision, 'Recall =', M_Recall)\n",
    "\n",
    "B_Precision = B_TP/(B_TP+B_FP)\n",
    "B_Recall = B_TP/(B_TP+B_FN)\n",
    "\n",
    "print('Category B' ,'Precision =', B_Precision, 'Recall =', B_Recall)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
